{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a26ab20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, List, Type, Tuple, Dict\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.axes._axes import Axes\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as D\n",
    "from torch.func import vmap, jacrev\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99853662",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampleable(ABC):\n",
    "    # distributions to be sampled from\n",
    "    def sample(self, num_samples: int) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - num_samples: the desired number of samples\n",
    "        Returns:\n",
    "            - samples: shape (batch_size, ...)\n",
    "            - labels: shape (batch_size, label_dim)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class IsotropicGaussian(nn.Module, Sampleable):\n",
    "    def __init__(self, shape: List[int], std: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "        self.std = std\n",
    "        self.dummy = nn.Buffer(torch.zeros(1))\n",
    "\n",
    "    def sample(self, num_samples) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        return self.std * torch.randn(num_samples, *self.shape).to(self.dummy.device), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e47480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalProbabilityPath(nn.Module, ABC):\n",
    "    def __init__(self, p_simple: Sampleable, p_data: Sampleable):\n",
    "        super().__init__()\n",
    "        self.p_simple = p_simple\n",
    "        self.p_data = p_data\n",
    "\n",
    "    @abstractmethod\n",
    "    def sample_conditioning_variable(self, num_samples: int) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        # get z and y\n",
    "        # returns: z, y = shape(num_samples, c, h, w), shape(num_samples, dim_label)\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def sample_conditional_path(self, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        # get position form the p_t(x|z), z: shape(bs, c, h, w), t: shape(bs, 1, 1, 1)\n",
    "        # returns: shape(bs, c, h, w)\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def conditional_vector_field(self, x: torch.Tensor, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        # get the velocity given position x, condition z and time t\n",
    "        # x: shape(bs, c, h, w), z: shape(bs, c, h, w), t: shape(bs, 1, 1, 1)\n",
    "        # returns: shape(bs, c, h, w)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a56f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alpha(ABC):\n",
    "    def __init__(self):\n",
    "        assert torch.allclose(self(torch.zeros(1, 1, 1, 1)), torch.zeros(1, 1, 1, 1))\n",
    "        assert torch.allclose(self(torch.ones(1, 1, 1, 1)), torch.ones(1, 1, 1, 1))\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def dt(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        t = t.unsqueeze(1)\n",
    "        return (vmap(jacrev(self)) (t)).view(-1, 1, 1, 1)\n",
    "    \n",
    "class Beta(ABC):\n",
    "    def __init__(self):\n",
    "        assert torch.allclose(self(torch.zeros(1, 1, 1, 1)), torch.ones(1, 1, 1, 1))\n",
    "        assert torch.allclose(self(torch.ones(1, 1, 1, 1)), torch.zeros(1, 1, 1, 1))\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def dt(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        return (vmap(jacrev(self)) (t.unsqueeze(1))).view(-1, 1, 1, 1)\n",
    "    \n",
    "class LinearAlpha(Alpha):\n",
    "    def __call__(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        # t: shape(num_samples, 1, 1, 1)\n",
    "        return t\n",
    "    \n",
    "    def dt(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        # t: shape(num_samples, 1, 1, 1)\n",
    "        return torch.ones_like(t)\n",
    "    \n",
    "class LinearBeta(Beta):\n",
    "    def __call__(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        # t: shape(num_samples, 1, 1, 1)\n",
    "        return 1 - t\n",
    "    \n",
    "    def dt(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        # t: shape(num_samples, 1, 1, 1)\n",
    "        return - torch.ones_like(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93bbb807",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianConditionalProbabilityPath(ConditionalProbabilityPath):\n",
    "    def __init__(self, p_data: Sampleable, p_simple_shape: List[int], alpha: Alpha, beta: Beta):\n",
    "        p_simple = IsotropicGaussian(shape=p_simple_shape, std=1.0)\n",
    "        super().__init__(p_data=p_data, p_simple=p_simple)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def sample_conditioning_variable(self, num_samples: int) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        return self.p_data.sample(num_samples)\n",
    "    \n",
    "    def sample_conditional_path(self, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        # z: shape(bs, c, h, w), t: shape(bs, 1, 1, 1)\n",
    "        return self.alpha(t) * z + self.beta(t) * torch.randn_like(z)\n",
    "\n",
    "    def conditional_vector_field(self, x: torch.Tensor, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        # x: shape(bs, c, h, w), z: shape(bs, c, h, w), t: shape(bs, c, h, w)\n",
    "        alpha_t = self.alpha(t)         # (bs, 1, 1, 1)\n",
    "        beta_t = self.beta(t)           # (bs, 1, 1, 1)\n",
    "        dt_alpha_t = self.alpha.dt(t)   # (bs, 1, 1, 1)\n",
    "        dt_beta_t = self.beta.dt(t)     # (bs, 1, 1, 1)\n",
    "\n",
    "        return (dt_alpha_t - dt_beta_t / beta_t * alpha_t) * z + dt_beta_t / beta_t * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "018e20fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorField(ABC): \n",
    "    @abstractmethod\n",
    "    def velocity(self, xt: torch.Tensor, t: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        # xt: the position at time t, shape(bs, c, h, w); t: shape(bs, 1, 1, 1)\n",
    "        # returns: the velocity shape(bs, c, h, w)\n",
    "        pass\n",
    "\n",
    "class Simulator(ABC):\n",
    "    @abstractmethod\n",
    "    def step(self, xt: torch.Tensor, t: torch.Tensor, dt: torch.Tensor, **kwargs):\n",
    "        \"\"\"\n",
    "        Takes one simulation step\n",
    "        Args:\n",
    "            - xt: state at time t, shape (bs, c, h, w)\n",
    "            - t: time, shape (bs, 1, 1, 1)\n",
    "            - dt: time, shape (bs, 1, 1, 1)\n",
    "        Returns:\n",
    "            - nxt: state at time t + dt (bs, c, h, w)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def simulate(self, x: torch.Tensor, ts: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Simulates using the discretization gives by ts\n",
    "        Args:\n",
    "            - x_init: initial state, shape (bs, c, h, w)\n",
    "            - ts: timesteps, shape (bs, nts, 1, 1, 1)\n",
    "        Returns:\n",
    "            - x_final: final state at time ts[-1], shape (bs, c, h, w)\n",
    "        \"\"\"\n",
    "        nts = ts.shape[1]\n",
    "        for t_idx in tqdm(range(nts - 1)):\n",
    "            t = ts[:, t_idx]\n",
    "            h = ts[:, t_idx + 1] - ts[:, t_idx]\n",
    "            x = self.step(x, t, h, **kwargs)\n",
    "        return x\n",
    "    \n",
    "class EulerSimulator(Simulator):\n",
    "    def __init__(self, vectorfield: VectorField):\n",
    "        self.vectorfield = vectorfield\n",
    "    \n",
    "    def step(self, xt: torch.Tensor, t: torch.Tensor, dt: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        return xt + self.vectorfield.velocity(xt, t, **kwargs) * dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac7bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MiB = 1024 ** 2\n",
    "\n",
    "def model_size_b(model: nn.Module) -> torch.Tensor:\n",
    "    size = 0\n",
    "    for param in model.parameters():\n",
    "        size += param.nelement() * param.element_size()\n",
    "    for buf in model.buffers():\n",
    "        size += buf.nelement() * buf.element_size()\n",
    "    return size\n",
    "\n",
    "class Trainer(ABC):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_train_loss(self, **kwargs) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def get_optimizer(self, lr: float):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "    \n",
    "    def train(self, num_epochs: int, device: torch.device, name: str, lr: float=1e-3, **kwargs):\n",
    "        # print the model size\n",
    "        size_b = model_size_b(self.model)\n",
    "        print(f'Training model with size: {size_b / MiB:.3f} MiB')\n",
    "\n",
    "        # start\n",
    "        self.model.to(device)\n",
    "        opt = self.get_optimizer(lr=lr)\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "\n",
    "        # train loop\n",
    "        pbar = tqdm(enumerate(range(num_epochs)))\n",
    "        for idx, epoch in pbar:\n",
    "            opt.zero_grad()\n",
    "            loss = self.get_train_loss(**kwargs)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            losses.append(loss.item())\n",
    "            pbar.set_description(f'Epoch {idx}, loss: {loss.item():.3f}')\n",
    "        \n",
    "        # finish\n",
    "        self.model.eval()\n",
    "\n",
    "        save_dir = \"./training_output\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # visualize the loss curve\n",
    "        plt.plot(losses)\n",
    "        plt.title(\"Training Loss Curve\")\n",
    "        plt.xlabel(\"num_epochs\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        if loss_name is not None:\n",
    "            os.makedirs(os.path.dirname(loss_name), exist_ok=True)\n",
    "            plt.savefig(loss_name, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "            print(f\"Loss curve saved to: {loss_name}\")\n",
    "        plt.show()\n",
    "\n",
    "        # save the parameters\n",
    "        torch.save(self.u_theta.state_dict(), self.save_addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d75664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
