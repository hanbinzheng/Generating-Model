{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78449923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 初始状态检查 ---\n",
      "X (输入数据): shape=torch.Size([5, 4]), requires_grad=False, grad_fn=None\n",
      "y_true (标签): shape=torch.Size([5]), requires_grad=False, grad_fn=None\n",
      "\n",
      "模型参数初始状态:\n",
      "  fc1.weight: shape=torch.Size([5, 4]), requires_grad=True, grad_fn=None, grad=None\n",
      "  fc1.bias: shape=torch.Size([5]), requires_grad=True, grad_fn=None, grad=None\n",
      "  fc2.weight: shape=torch.Size([10, 5]), requires_grad=True, grad_fn=None, grad=None\n",
      "  fc2.bias: shape=torch.Size([10]), requires_grad=True, grad_fn=None, grad=None\n",
      "--------------------------------------------------\n",
      "\n",
      "--- 前向传播开始：一步步看 grad_fn 的变化 ---\n",
      "所有模型参数的 .grad 已清零。\n",
      "\n",
      "fc1_output (X @ W1.T + b1): shape=torch.Size([5, 5])\n",
      "  requires_grad=True\n",
      "  grad_fn=<AddmmBackward0 object at 0x7816058016f0>\n",
      "\n",
      "relu_output (ReLU(fc1_output)): shape=torch.Size([5, 5])\n",
      "  requires_grad=True\n",
      "  grad_fn=<ReluBackward0 object at 0x7816055b14e0>\n",
      "\n",
      "logits (最终输出): shape=torch.Size([5, 10])\n",
      "  requires_grad=True\n",
      "  grad_fn=<AddmmBackward0 object at 0x7816058016f0>\n",
      "\n",
      "Loss (交叉熵损失): shape=torch.Size([])\n",
      "  requires_grad=True\n",
      "  grad_fn=<NllLossBackward0 object at 0x7816055b14e0>\n",
      "\n",
      "此时，一个从 loss 回溯到 W1, b1, W2, b2 的计算图已动态构建完成。\n",
      "--------------------------------------------------\n",
      "\n",
      "--- 反向传播开始：从 Loss 回溯计算梯度 ---\n",
      "\n",
      "反向传播完成！检查参数的 .grad 属性。\n",
      "  fc1.weight:\n",
      "    - grad_fn: None\n",
      "    - grad is None: False\n",
      "    - grad.shape: torch.Size([5, 4])\n",
      "  fc1.bias:\n",
      "    - grad_fn: None\n",
      "    - grad is None: False\n",
      "    - grad.shape: torch.Size([5])\n",
      "  fc2.weight:\n",
      "    - grad_fn: None\n",
      "    - grad is None: False\n",
      "    - grad.shape: torch.Size([10, 5])\n",
      "  fc2.bias:\n",
      "    - grad_fn: None\n",
      "    - grad is None: False\n",
      "    - grad.shape: torch.Size([10])\n",
      "--------------------------------------------------\n",
      "\n",
      "--- 参数更新阶段：优化器利用 .grad 更新参数 ---\n",
      "更新前 model.fc1.weight[0,0]: -0.3402\n",
      "更新前 model.fc2.bias[0]: 0.0619\n",
      "\n",
      "优化器更新完成！参数值已改变，.grad 已清零。\n",
      "  fc1.weight:\n",
      "    - grad is None: False\n",
      "  fc1.bias:\n",
      "    - grad is None: False\n",
      "  fc2.weight:\n",
      "    - grad is None: False\n",
      "  fc2.bias:\n",
      "    - grad is None: False\n",
      "更新后 model.fc1.weight[0,0]: -0.3393\n",
      "更新后 model.fc2.bias[0]: 0.0608\n",
      "fc1.weight[0,0] 变化量: 0.0009\n",
      "fc2.bias[0] 变化量: -0.0010\n",
      "\n",
      "--- 一个完整的训练步骤完成 ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 确保在 CPU 上运行，以便观察内存地址等细节\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# --- 1. 定义模型和数据 ---\n",
    "# 简化版两层 MLP\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # fc1 层：输入 input_dim，输出 hidden_dim\n",
    "        # nn.Linear 会自动创建 weight 和 bias\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # 激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # fc2 层：输入 hidden_dim，输出 output_dim\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> fc1 -> relu -> fc2 -> output\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 模拟数据\n",
    "input_dim = 4      # 每张图片的特征数（比如 2x2 展平后）\n",
    "hidden_dim = 5     # 隐藏层神经元数\n",
    "output_dim = 10    # 10 个数字类别 (0-9)\n",
    "batch_size = 5     # 5 张图片\n",
    "\n",
    "# 模拟输入数据 X，不需要求导\n",
    "X = torch.randn(batch_size, input_dim, requires_grad=False).to(device)\n",
    "# 模拟标签 y\n",
    "y_true = torch.randint(0, output_dim, (batch_size,), dtype=torch.long).to(device)\n",
    "\n",
    "# 实例化模型\n",
    "model = SimpleMLP(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"--- 初始状态检查 ---\")\n",
    "print(f\"X (输入数据): shape={X.shape}, requires_grad={X.requires_grad}, grad_fn={X.grad_fn}\")\n",
    "print(f\"y_true (标签): shape={y_true.shape}, requires_grad={y_true.requires_grad}, grad_fn={y_true.grad_fn}\\n\")\n",
    "\n",
    "# 检查模型的参数 W1, b1, W2, b2\n",
    "# model.named_parameters() 返回参数名称和参数张量本身\n",
    "print(\"模型参数初始状态:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: shape={param.shape}, requires_grad={param.requires_grad}, grad_fn={param.grad_fn}, grad={param.grad}\")\n",
    "    # 注意：所有参数的 requires_grad 都是 True，因为它们需要被优化\n",
    "    # grad_fn 都是 None，因为它们是“叶子节点”，是直接创建的，不是通过运算得来。\n",
    "    # grad 都是 None，因为还没有进行反向传播。\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 2. 前向传播：构建计算图 ---\n",
    "print(\"\\n--- 前向传播开始：一步步看 grad_fn 的变化 ---\")\n",
    "\n",
    "# 步骤 2.0: 清零梯度 (习惯上，虽然这里是第一次，但模型参数可能已经有历史梯度)\n",
    "optimizer.zero_grad()\n",
    "print(\"所有模型参数的 .grad 已清零。\")\n",
    "\n",
    "# 步骤 2.1: 通过 fc1 层\n",
    "# x 是 (5, 4)， self.fc1.weight 是 (hidden_dim, input_dim) 即 (5, 4)\n",
    "# 矩阵乘法 X @ W_transpose + b\n",
    "# (5, 4) @ (4, 5) -> (5, 5) + (5,) -> (5, 5)\n",
    "fc1_output = model.fc1(X)\n",
    "print(f\"\\nfc1_output (X @ W1.T + b1): shape={fc1_output.shape}\")\n",
    "print(f\"  requires_grad={fc1_output.requires_grad}\") # True\n",
    "# 此时 fc1_output 的 grad_fn 会是一个代表线性层操作的 Function\n",
    "# (如 AddmmBackward0，因为它包含了矩阵乘法和加法)\n",
    "print(f\"  grad_fn={fc1_output.grad_fn}\")\n",
    "# fc1_output 内部的 grad_fn 会记住：\n",
    "#   - 它是由 model.fc1 这个 nn.Linear 模块产生的。\n",
    "#   - 它需要 model.fc1.weight 和 model.fc1.bias 来计算梯度。\n",
    "#   - 它会存储 X（输入）的引用，以便计算 X 的梯度（如果 X 允许求导）。\n",
    "\n",
    "\n",
    "# 步骤 2.2: 通过 ReLU 激活函数\n",
    "relu_output = model.relu(fc1_output)\n",
    "print(f\"\\nrelu_output (ReLU(fc1_output)): shape={relu_output.shape}\")\n",
    "print(f\"  requires_grad={relu_output.requires_grad}\") # True\n",
    "# relu_output 的 grad_fn 会是一个代表 ReLU 操作的 Function\n",
    "print(f\"  grad_fn={relu_output.grad_fn}\") # 例如 <ReLUBackward0 object at ...>\n",
    "# relu_output 内部的 grad_fn 会记住：\n",
    "#   - 它的输入是 fc1_output。\n",
    "#   - 它会存储 fc1_output 的引用以及在 ReLU 操作中哪些值是正的（用于梯度计算）。\n",
    "\n",
    "\n",
    "# 步骤 2.3: 通过 fc2 层 (最终输出 logits)\n",
    "# relu_output 是 (5, 5)， self.fc2.weight 是 (output_dim, hidden_dim) 即 (10, 5)\n",
    "# (5, 5) @ (5, 10) -> (5, 10) + (10,) -> (5, 10)\n",
    "logits = model.fc2(relu_output)\n",
    "print(f\"\\nlogits (最终输出): shape={logits.shape}\")\n",
    "print(f\"  requires_grad={logits.requires_grad}\") # True\n",
    "# logits 的 grad_fn 会是一个代表线性层操作的 Function\n",
    "print(f\"  grad_fn={logits.grad_fn}\") # 例如 <AddmmBackward0 object at ...>\n",
    "# logits 内部的 grad_fn 会记住：\n",
    "#   - 它是通过 model.fc2 产生的。\n",
    "#   - 它需要 model.fc2.weight 和 model.fc2.bias 来计算梯度。\n",
    "#   - 它会存储 relu_output 的引用。\n",
    "\n",
    "\n",
    "# 步骤 2.4: 计算损失 (交叉熵)\n",
    "# criterion(logits, y_true)\n",
    "# CrossEntropyLoss 内部会先对 logits 进行 LogSoftmax，再进行 NLLLoss\n",
    "loss = criterion(logits, y_true)\n",
    "print(f\"\\nLoss (交叉熵损失): shape={loss.shape}\")\n",
    "print(f\"  requires_grad={loss.requires_grad}\") # True\n",
    "# 损失张量通常是标量 (shape=torch.Size([])), 它的 grad_fn 会指向损失函数的反向计算\n",
    "print(f\"  grad_fn={loss.grad_fn}\") # 例如 <NllLossBackward0 object at ...>\n",
    "# loss 内部的 grad_fn 会记住：\n",
    "#   - 它的输入是 logits 和 y_true。\n",
    "#   - 它需要 logits 的值和 y_true 的值来计算梯度。\n",
    "\n",
    "print(\"\\n此时，一个从 loss 回溯到 W1, b1, W2, b2 的计算图已动态构建完成。\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 3. 反向传播：计算梯度 ---\n",
    "print(\"\\n--- 反向传播开始：从 Loss 回溯计算梯度 ---\")\n",
    "\n",
    "# 调用 backward() 方法\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n反向传播完成！检查参数的 .grad 属性。\")\n",
    "# 检查模型参数的梯度\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    - grad_fn: {param.grad_fn}\") # 仍是 None，因为它们是叶子节点\n",
    "    print(f\"    - grad is None: {param.grad is None}\")\n",
    "    if param.grad is not None:\n",
    "        print(f\"    - grad.shape: {param.grad.shape}\")\n",
    "        # 梯度值：是 loss 对该参数中每个元素的偏导数\n",
    "        # 它们是和参数形状相同的 Tensor\n",
    "        # print(f\"    - grad value: \\n{param.grad}\") # 值比较大，不打印了，但你可以自行打印\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 4. 参数更新：优化器根据梯度更新参数 ---\n",
    "print(\"\\n--- 参数更新阶段：优化器利用 .grad 更新参数 ---\")\n",
    "\n",
    "# 获取参数更新前的值 (只取一部分为例)\n",
    "old_fc1_weight_val = model.fc1.weight.data[0, 0].item()\n",
    "old_fc2_bias_val = model.fc2.bias.data[0].item()\n",
    "print(f\"更新前 model.fc1.weight[0,0]: {old_fc1_weight_val:.4f}\")\n",
    "print(f\"更新前 model.fc2.bias[0]: {old_fc2_bias_val:.4f}\")\n",
    "\n",
    "optimizer.step() # 优化器执行一步更新\n",
    "\n",
    "print(\"\\n优化器更新完成！参数值已改变，.grad 已清零。\")\n",
    "# 检查参数更新后的值和梯度状态\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}:\")\n",
    "    # 梯度已经被清零了 (通常是 None，或者一个全零张量，取决于优化器实现)\n",
    "    print(f\"    - grad is None: {param.grad is None}\")\n",
    "\n",
    "# 获取参数更新后的值\n",
    "new_fc1_weight_val = model.fc1.weight.data[0, 0].item()\n",
    "new_fc2_bias_val = model.fc2.bias.data[0].item()\n",
    "print(f\"更新后 model.fc1.weight[0,0]: {new_fc1_weight_val:.4f}\")\n",
    "print(f\"更新后 model.fc2.bias[0]: {new_fc2_bias_val:.4f}\")\n",
    "print(f\"fc1.weight[0,0] 变化量: {new_fc1_weight_val - old_fc1_weight_val:.4f}\")\n",
    "print(f\"fc2.bias[0] 变化量: {new_fc2_bias_val - old_fc2_bias_val:.4f}\")\n",
    "\n",
    "# 可以重复整个循环（前向传播、反向传播、参数更新）来进行多轮训练\n",
    "print(\"\\n--- 一个完整的训练步骤完成 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f2b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
